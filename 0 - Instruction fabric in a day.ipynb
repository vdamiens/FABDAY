{"cells":[{"cell_type":"markdown","source":["![image-alt-text](https://www.lebigdata.fr/wp-content/uploads/2021/04/claranet-data-modernisation-tout-savoir.jpg)\n","# Notebook d'instruction Fabric in a day Claranet \n","### Ce notebook contient les instructions du hands-on Fabric Claranet\n","###### Auteur: Vincent DAMIENS\n","garder ce notebook ouvert pour suivre les étapes du hands-on. \\\n","On fera un point à chaque étape pour revenir sur les actions réalisés.\n","\n","***"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fffa32dc-1e83-4285-b31b-a3a004655220"},{"cell_type":"markdown","source":["## Etape 1: initialisation des accès\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf41010d-95f9-4fea-9337-8d567639b8b0"},{"cell_type":"markdown","source":["Vous avez reçu un email avec votre compte d'accès à l'environnement de hands-on.\\\n","Celui-ci contient un compte en <mark>****\"@clarafr.onmicrosoft.com\"</mark>\\\n","Vous pouvez tester la connexion à fabric en suivant le lien suivant: https://app.fabric.microsoft.com/ "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81447e49-eafb-467e-a32e-b90cb360ce0c"},{"cell_type":"markdown","source":["Lors de votre connexion vous devez dans le menu espace de travail sur le bandeau de gauche avoir accès à 2 espace de travail, \\\n","un espace contenant votre compte utilisateur et un autre \"Claranet Fabric in a day\" \\\n","\n","***"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a1eee14c-a685-4e11-bd66-ff6d1f63e7ab"},{"cell_type":"markdown","source":["## Etape 2: Construction d'un lakehouse \n","\\ ![image-alt-text](https://debruyn.dev/2023/all-microsoft-fabric-icons-for-diagramming-old-version/Lakehouse.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"94bf8a6a-fb91-4e63-acef-3145bc738c5c"},{"cell_type":"markdown","source":["Dans cette section, vous allez créer un \"lakehouse\" dans Fabric.\n","\n","- Dans Fabric, sélectionnez Espaces de travail dans la barre de navigation.\n","- Pour ouvrir votre espace de travail, entrez son nom dans la zone de recherche située en haut et sélectionnez-le dans les résultats de la recherche.\n","- Dans l’espace de travail, sélectionnez Nouvel élément, puis Lakehouse.\n","- Dans la boîte de dialogue Nouveau lakehouse , entrez <mark>wwilakehouse</mark> dans le champ Nom.\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/new-lakehouse-name.png)\n","***\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcee6193-e117-43b7-a561-3dd4f7ed84b9"},{"cell_type":"markdown","source":["## Etape 3: Alimentation de données"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9472d332-d544-45a8-a57f-d1095ec1d253"},{"cell_type":"markdown","source":["Une fois le lakehouse créer nous allons y intégré des données d'exemples.\\\n","Téléchargez le fichier dimension_customer.csv à partir du référentiel d’exemples Fabric: https://github.com/microsoft/fabric-samples/blob/689e78676174d4627fc3855165bde9100cb4d19e/docs-samples/data-engineering/dimension_customer.csv\n","\\\n","Ce fichier devra être déposer dans le lakehouse nouvellement créé.\\\n","- Pour ce faire, se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>\n","- Une fois dans l'explorateur de lakehouse, dans le menu bandeau, se rendre sur \"Obtenir des données\" puis \"Charger des fichiers\"\n","- dans le menu qui vient de s'ouvrir charger de fichier d'exemple téléchargé précédement"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7595ec59-fb64-4531-ae80-9f8f5a39d410"},{"cell_type":"markdown","source":["Une fois le fichier dans le onelake retourner dans le menu bandeau,\"Obtenir des données\" puis \"Nouveau flux de données Gen2\"\n","- se rendre dans \"obtenir des données d'une autre source\"\n","- dans le menu d'import dans le catalogue onelake choisir le lakehouse <mark>wwilakehouse</mark>\n","- de le menu de choix de données dans fichier choisir le fichier dimension_customer.csv, puis créer\n","- Dans le volet Paramètres de la requête, mettez à jour le champ Nom avec dimension_customer  \n","- Dans le menu bandeau choisir transformer puis \"Utiliser la première ligne pour les en-têtes\"\n","\n","Dans les éléments de menu, sélectionnez Ajouter une destination de données, puis lakehouse. \\\n","À partir de l’écran Se connecter à la destination des données, connectez-vous à votre compte si nécessaire et sélectionnez Suivant.\n","\n","Accédez à la <mark>wwilakehouse</mark> de votre espace de travail. \\\n","sélectionnez le paramètre Nouvelle table et entrez le nom de la table dimension_customer.\\\n","Cliquez sur Suivant. \n","\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/query-settings-add-destination.png#lightbox)\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/choose-destination-table.png)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"716f0da2-0e11-432f-9978-c19423af1c82"},{"cell_type":"markdown","source":["- Dans le volet Choisir les paramètres de destination , sélectionnez Remplacer comme méthode Mise à jour. \n","- Sélectionnez Enregistrer les paramètres pour revenir au canevas du flux de données.\n","- Pour continuer, sélectionnez Publier en bas à droite de l’écran.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b581626f-ee9c-408b-a1dc-8e452135b715"},{"cell_type":"markdown","source":["Un cercle tournant en regard du nom du flux de données indique que la publication est en cours dans la vue d’élément. \\\n","Une fois la publication terminée, sélectionnez ... et sélectionnez Propriétés. \\\n","Renommez le flux de données en Charger la table Lakehouse et sélectionnez <mark>Enregistrer</mark>."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac2ac0ed-cddb-4e22-9f13-f5eb74483e88"},{"cell_type":"markdown","source":["Sélectionnez l’option Actualiser maintenant en regard du nom du flux de données pour actualiser le flux de données. \\\n","Cette option exécute le flux de données et déplace les données du fichier source vers la table lakehouse. \\\n","Pendant qu’il est en cours, vous voyez un cercle tournant sous colonne Actualisée dans l’affichage d’élément. \n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/dataflow-refresh-now.png)\n","\n","***"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5da6541-923f-4095-86a0-b262c9b6ea63"},{"cell_type":"markdown","source":["## Etape 4: Requetage du lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5dbae409-58fd-4c35-acdc-5f57bfbb5c43"},{"cell_type":"markdown","source":["Une fois le flux de données actualisé, sélectionnez votre nouveau lakehouse dans la barre de navigation pour afficher la table Delta dimension_customer.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/open-lakehouse.png)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1641c167-354d-4e5f-8c7c-29523a949fb7"},{"cell_type":"markdown","source":["Sélectionnez la table pour afficher un aperçu de ses données. \\\n","Vous pouvez également utiliser le point de terminaison d’analytique SQL du lakehouse pour interroger les données avec des instructions SQL. \\\n","Sélectionnez point de terminaison d’analytique SQL dans le menu déroulant Lakehouse en haut à droite de l’écran.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/lakehouse-delta-table.png#lightbox)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6be82073-6903-4340-8cc7-4c4441930b44"},{"cell_type":"markdown","source":["Sélectionnez la table dimension_customer pour afficher un aperçu de ses données ou sélectionnez Nouvelle requête SQL pour écrire vos instructions SQL.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/warehouse-mode-new-sql.png)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c28e136-eb86-4e49-9e23-6234afb6c6c7"},{"cell_type":"markdown","source":["L’exemple de requête suivant agrège le nombre de lignes en fonction de la colonne BuyingGroup de la table dimension_customer. \\\n","Les fichiers de requête SQL sont enregistrés automatiquement pour référence ultérieure, et vous pouvez renommer ou supprimer ces fichiers en fonction de vos besoins. \n","\n","Pour exécuter le script, sélectionnez l’icône Exécuter en haut du fichier de script.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a27e4308-e78e-4edc-ba7c-aa11a12c4824"},{"cell_type":"code","source":["SELECT BuyingGroup, Count(*) AS Total\n","FROM dimension_customer\n","GROUP BY BuyingGroup"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21909682-83f7-4993-90c8-666a89dc142e"},{"cell_type":"markdown","source":["***"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386570d3-4cc3-4157-bf40-3ab6202e2017"},{"cell_type":"markdown","source":["## Etape 5: création d'un rapport\n","Dans cette section, vous allez générer un rapport à partir des données ingérées.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8686420b-5870-4189-86c3-91a67d83e3be"},{"cell_type":"markdown","source":["Auparavant, toutes les tables et vues de Lakehouse étaient automatiquement ajoutées au modèle sémantique. \\\n","Avec les récentes mises à jour, pour les nouvelles maisons de lac, vous devez ajouter manuellement vos tables au modèle sémantique. \\\n","Ouvrez votre lakehouse et basculez vers la vue du Point de terminaison d’analytique SQL. \\\n","Dans l'onglet Reporting, sélectionnez Gérer le modèle sémantique par défaut et sélectionnez les tables que vous souhaitez ajouter au modèle sémantique. \\\n","Dans ce cas, sélectionnez la table dimension_customer.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/select-semantic-model-tables.png)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6c5e188-60f1-42a8-8b76-0709d3619219"},{"cell_type":"markdown","source":["our vous assurer que les tables du modèle sémantique sont toujours synchronisées, passez à la vue du point de terminaison Analytique SQL et ouvrez le volet paramètres de l'entrepôt de données. \\\n","Sélectionnez Modèle sémantique Power BI par défaut et activez Synchroniser le modèle sémantique Power BI par défaut. \\\n","Pour plus d’informations, consultez Modèles sémantiques Power BI par défaut.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/enable-semantic-model-sync.png)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"222f8834-4fc1-4cce-82fe-741ba60ab8a0"},{"cell_type":"markdown","source":["Une fois la table ajoutée, Fabric crée un modèle sémantique portant le même nom que le lakehouse.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/workspace-default-dataset.png)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"131b8030-0069-452b-afaf-3941155189f2"},{"cell_type":"markdown","source":["Dans le volet de modèle sémantique, vous pouvez afficher toutes les tables. \\\n","Vous disposez d’options pour créer des rapports à partir de zéro, des rapports paginés ou laisser Power BI créer automatiquement un rapport en fonction de vos données. \\\n","Dans le cadre de ce tutoriel, sous Explorer ces données, sélectionnez Créer automatiquement un rapport. \\\n","Dans le tutoriel suivant, nous créons un rapport à partir de zéro.\n","\n","\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/dataset-details-create-report.png)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2c3e939-6b2a-4ea7-a51e-ddcd9bb71571"},{"cell_type":"markdown","source":["Étant donné que la table est une dimension et qu’il n’y a aucune mesure, Power BI crée une mesure pour le nombre de lignes et l’agrège sur différentes colonnes, et crée différents graphiques, comme illustré dans l’image suivante. \\\n","Vous pouvez enregistrer ce rapport pour l’avenir en sélectionnant Enregistrer dans le ruban supérieur. \\\n","Vous pouvez apporter d’autres modifications à ce rapport pour répondre à vos besoins en incluant ou en excluant d’autres tables ou colonnes.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/quick-summary-report.png)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"660918f5-3762-4ae8-a11f-8575d28b87a8"},{"cell_type":"markdown","source":["***\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0481943e-9eb3-4493-8fb7-baec092e72d9"},{"cell_type":"markdown","source":["## Etape 6: Ingestion de données massive\n","\n","<mark>  Point important nous ne ferons pas l'import de donnéees complet afin d'eviter un traffic trop massif lors de l'import en parallèle de toutes les personnes présentent. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d22b163b-62f9-41e1-af5e-585a930c3390"},{"cell_type":"markdown","source":["Dans cette section, vous allez utiliser l’activité Copier des données du pipeline Data Factory pour ingérer des exemples de données d’un compte de stockage Azure vers la section Fichiers du lakehouse que vous avez créé précédemment.\n","\n","1.  Pour ce faire, se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>\n","2. Une fois dans l'explorateur de lakehouse, dans le menu bandeau, se rendre sur \"Obtenir des données\" puis \"Nouveau pipeline de données\"\n","3. Ensuite, configurez une connexion HTTP pour importer les exemples de données World Wide Importers dans le lakehouse. Dans la liste des Nouvelles sources, sélectionnez Afficher plus, recherchez HTTP et sélectionnez-le\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/select-http-connection.png)\n","\n","4. Dans la fenêtre Se connecter à la source de données, entrez les détails du tableau ci-dessous, puis sélectionnez Suivant.\n","\n","| Propriété | Valeur |\n","| :--------- |:---------|\n","| URL\t | https://assetsprod.microsoft.com/en-us/wwi-sample-dataset.zip |\n","| Connexion | Créer une connexion |\n","|Nom de la connexion |\twwisampledata |\n","|Passerelle de données |\tAucun(e) |\n","|Type d'authentification |\tAnonyme |\n","\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/configure-http-connection.png)\n","\n","5. À l’étape suivante, activez la copie binaire et choisissez ZipDeflate (.zip) comme Type de compression, car la source est un fichier .zip. \\\n","Conservez les valeurs par défaut des autres champs, puis cliquez sur Suivant.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/select-compression-type.png)\n","\n","6. Dans la fenêtre Se connecter à la destination des données, spécifiez Fichiers comme Dossier racine, puis sélectionnez Suivant. Cela écrit les données dans la section Fichiers du lakehouse.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/configure-destination-connection.png)\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8234891-4b1a-409e-8bed-8b800f55d2b4"},{"cell_type":"markdown","source":["7. Choisissez Binaire comme Format de fichier pour la destination. Sélectionnez Suivant, puis décocher la check box \"Démarrer le transfert de données immédiatement\", et validé avec \"OK\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"804630d8-3f82-4816-a924-d57e4ce8a038"},{"cell_type":"markdown","source":["***\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8375a316-2203-4207-bcd9-1f3db34b7fba"},{"cell_type":"markdown","source":["## Etape 7: Création d'un shortcut"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"47d52561-90ce-4e0e-a5c4-dbc5abbcfed9"},{"cell_type":"markdown","source":["Afin d'éviter l'import massif de données, vous allez mettre en place un shortcut vers le Onelake \"Fabric in a day\" \n","\n","Pour se faire se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>. \\\n","Dans l'explorateur dans le dossier files, faire un clic-droit, puis \"Nouveau raccourci\" \\\n","Dans le choix des sources externes, choisir \"Microsoft Onelake\"  \\\n","Choisir le lake nommée <mark>wwilakehouse</mark> qui est dans l'emplacement \"Claranet Fabric in a day\" \\\n","Dans l'explorateur choisir \"Files\", puis \" wwi-raw-data\" et cocher le dossier \"wwi-raw-data\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b96537f-18ab-4281-b8c5-04cbbbc00771"},{"cell_type":"markdown","source":["Vérifiez que le dossier WideWorldImportersDW est présent dans la vue Explorateur et contient des données pour toutes les tables.\n","\n","![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/validate-destination-files.png)\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"991258b0-f5c5-4048-8f2b-f402082aec74"},{"cell_type":"markdown","source":["***\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f5e1322-9afa-4d06-8b37-bbd07ea86e83"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"fr"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}