{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffa32dc-1e83-4285-b31b-a3a004655220",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "![image-alt-text](https://www.lebigdata.fr/wp-content/uploads/2021/04/claranet-data-modernisation-tout-savoir.jpg)\n",
    "# Notebook d'instruction Fabric in a day Claranet \n",
    "### Ce notebook contient les instructions du hands-on Fabric Claranet\n",
    "###### Auteur: Vincent DAMIENS\n",
    "garder ce notebook ouvert pour suivre les étapes du hands-on. \\\n",
    "On fera un point à chaque étape pour revenir sur les actions réalisés.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41010d-95f9-4fea-9337-8d567639b8b0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 1: initialisation des accès\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81447e49-eafb-467e-a32e-b90cb360ce0c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Vous avez reçu un email avec votre compte d'accès à l'environnement de hands-on.\\\n",
    "Celui-ci contient un compte en <mark>****\"@clarafr.onmicrosoft.com\"</mark>\\\n",
    "Vous pouvez tester la connexion à fabric en suivant le lien suivant: https://app.fabric.microsoft.com/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eee14c-a685-4e11-bd66-ff6d1f63e7ab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Lors de votre connexion vous devez dans le menu espace de travail sur le bandeau de gauche avoir accès à 2 espace de travail, \\\n",
    "un espace contenant votre compte utilisateur et un autre \"Claranet Fabric in a day\" \\\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf8a6a-fb91-4e63-acef-3145bc738c5c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 2: Construction d'un lakehouse \n",
    "\\ ![image-alt-text](https://debruyn.dev/2023/all-microsoft-fabric-icons-for-diagramming-old-version/Lakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee6193-e117-43b7-a561-3dd4f7ed84b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Dans cette section, vous allez créer un \"lakehouse\" dans Fabric.\n",
    "\n",
    "- Dans Fabric, sélectionnez Espaces de travail dans la barre de navigation.\n",
    "- Pour ouvrir votre espace de travail, entrez son nom dans la zone de recherche située en haut et sélectionnez-le dans les résultats de la recherche.\n",
    "- Dans l’espace de travail, sélectionnez Nouvel élément, puis Lakehouse.\n",
    "- Dans la boîte de dialogue Nouveau lakehouse , entrez <mark>wwilakehouse</mark> dans le champ Nom.\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/new-lakehouse-name.png)\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472d332-d544-45a8-a57f-d1095ec1d253",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 3: Alimentation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595ec59-fb64-4531-ae80-9f8f5a39d410",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Une fois le lakehouse créer nous allons y intégré des données d'exemples.\\\n",
    "Téléchargez le fichier dimension_customer.csv à partir du référentiel d’exemples Fabric: https://github.com/microsoft/fabric-samples/blob/689e78676174d4627fc3855165bde9100cb4d19e/docs-samples/data-engineering/dimension_customer.csv\n",
    "\\\n",
    "Ce fichier devra être déposer dans le lakehouse nouvellement créé.\\\n",
    "- Pour ce faire, se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>\n",
    "- Une fois dans l'explorateur de lakehouse, dans le menu bandeau, se rendre sur \"Obtenir des données\" puis \"Charger des fichiers\"\n",
    "- dans le menu qui vient de s'ouvrir charger de fichier d'exemple téléchargé précédement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f0da2-0e11-432f-9978-c19423af1c82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Une fois le fichier dans le onelake retourner dans le menu bandeau,\"Obtenir des données\" puis \"Nouveau flux de données Gen2\"\n",
    "- se rendre dans \"obtenir des données d'une autre source\"\n",
    "- dans le menu d'import dans le catalogue onelake choisir le lakehouse <mark>wwilakehouse</mark>\n",
    "- de le menu de choix de données dans fichier choisir le fichier dimension_customer.csv, puis créer\n",
    "- Dans le volet Paramètres de la requête, mettez à jour le champ Nom avec dimension_customer  \n",
    "- Dans le menu bandeau choisir transformer puis \"Utiliser la première ligne pour les en-têtes\"\n",
    "\n",
    "Dans les éléments de menu, sélectionnez Ajouter une destination de données, puis lakehouse. \\\n",
    "À partir de l’écran Se connecter à la destination des données, connectez-vous à votre compte si nécessaire et sélectionnez Suivant.\n",
    "\n",
    "Accédez à la <mark>wwilakehouse</mark> de votre espace de travail. \\\n",
    "sélectionnez le paramètre Nouvelle table et entrez le nom de la table dimension_customer.\\\n",
    "Cliquez sur Suivant. \n",
    "\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/query-settings-add-destination.png#lightbox)\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/choose-destination-table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581626f-ee9c-408b-a1dc-8e452135b715",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- Dans le volet Choisir les paramètres de destination , sélectionnez Remplacer comme méthode Mise à jour. \n",
    "- Sélectionnez Enregistrer les paramètres pour revenir au canevas du flux de données.\n",
    "- Pour continuer, sélectionnez Publier en bas à droite de l’écran.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ac0ed-cddb-4e22-9f13-f5eb74483e88",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Un cercle tournant en regard du nom du flux de données indique que la publication est en cours dans la vue d’élément. \\\n",
    "Une fois la publication terminée, sélectionnez ... et sélectionnez Propriétés. \\\n",
    "Renommez le flux de données en Charger la table Lakehouse et sélectionnez <mark>Enregistrer</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da6541-923f-4095-86a0-b262c9b6ea63",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Sélectionnez l’option Actualiser maintenant en regard du nom du flux de données pour actualiser le flux de données. \\\n",
    "Cette option exécute le flux de données et déplace les données du fichier source vers la table lakehouse. \\\n",
    "Pendant qu’il est en cours, vous voyez un cercle tournant sous colonne Actualisée dans l’affichage d’élément. \n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/dataflow-refresh-now.png)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbae409-58fd-4c35-acdc-5f57bfbb5c43",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 4: Requetage du lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641c167-354d-4e5f-8c7c-29523a949fb7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Une fois le flux de données actualisé, sélectionnez votre nouveau lakehouse dans la barre de navigation pour afficher la table Delta dimension_customer.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/open-lakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be82073-6903-4340-8cc7-4c4441930b44",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Sélectionnez la table pour afficher un aperçu de ses données. \\\n",
    "Vous pouvez également utiliser le point de terminaison d’analytique SQL du lakehouse pour interroger les données avec des instructions SQL. \\\n",
    "Sélectionnez point de terminaison d’analytique SQL dans le menu déroulant Lakehouse en haut à droite de l’écran.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/lakehouse-delta-table.png#lightbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28e136-eb86-4e49-9e23-6234afb6c6c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Sélectionnez la table dimension_customer pour afficher un aperçu de ses données ou sélectionnez Nouvelle requête SQL pour écrire vos instructions SQL.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/warehouse-mode-new-sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e4308-e78e-4edc-ba7c-aa11a12c4824",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "L’exemple de requête suivant agrège le nombre de lignes en fonction de la colonne BuyingGroup de la table dimension_customer. \\\n",
    "Les fichiers de requête SQL sont enregistrés automatiquement pour référence ultérieure, et vous pouvez renommer ou supprimer ces fichiers en fonction de vos besoins. \n",
    "\n",
    "Pour exécuter le script, sélectionnez l’icône Exécuter en haut du fichier de script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21909682-83f7-4993-90c8-666a89dc142e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "SELECT BuyingGroup, Count(*) AS Total\n",
    "FROM dimension_customer\n",
    "GROUP BY BuyingGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386570d3-4cc3-4157-bf40-3ab6202e2017",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686420b-5870-4189-86c3-91a67d83e3be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 5: création d'un rapport\n",
    "Dans cette section, vous allez générer un rapport à partir des données ingérées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5e188-60f1-42a8-8b76-0709d3619219",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Auparavant, toutes les tables et vues de Lakehouse étaient automatiquement ajoutées au modèle sémantique. \\\n",
    "Avec les récentes mises à jour, pour les nouvelles maisons de lac, vous devez ajouter manuellement vos tables au modèle sémantique. \\\n",
    "Ouvrez votre lakehouse et basculez vers la vue du Point de terminaison d’analytique SQL. \\\n",
    "Dans l'onglet Reporting, sélectionnez Gérer le modèle sémantique par défaut et sélectionnez les tables que vous souhaitez ajouter au modèle sémantique. \\\n",
    "Dans ce cas, sélectionnez la table dimension_customer.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/select-semantic-model-tables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f8834-4fc1-4cce-82fe-741ba60ab8a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "our vous assurer que les tables du modèle sémantique sont toujours synchronisées, passez à la vue du point de terminaison Analytique SQL et ouvrez le volet paramètres de l'entrepôt de données. \\\n",
    "Sélectionnez Modèle sémantique Power BI par défaut et activez Synchroniser le modèle sémantique Power BI par défaut. \\\n",
    "Pour plus d’informations, consultez Modèles sémantiques Power BI par défaut.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/enable-semantic-model-sync.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b8030-0069-452b-afaf-3941155189f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Une fois la table ajoutée, Fabric crée un modèle sémantique portant le même nom que le lakehouse.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/workspace-default-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3e939-6b2a-4ea7-a51e-ddcd9bb71571",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Dans le volet de modèle sémantique, vous pouvez afficher toutes les tables. \\\n",
    "Vous disposez d’options pour créer des rapports à partir de zéro, des rapports paginés ou laisser Power BI créer automatiquement un rapport en fonction de vos données. \\\n",
    "Dans le cadre de ce tutoriel, sous Explorer ces données, sélectionnez Créer automatiquement un rapport. \\\n",
    "Dans le tutoriel suivant, nous créons un rapport à partir de zéro.\n",
    "\n",
    "\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/dataset-details-create-report.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660918f5-3762-4ae8-a11f-8575d28b87a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Étant donné que la table est une dimension et qu’il n’y a aucune mesure, Power BI crée une mesure pour le nombre de lignes et l’agrège sur différentes colonnes, et crée différents graphiques, comme illustré dans l’image suivante. \\\n",
    "Vous pouvez enregistrer ce rapport pour l’avenir en sélectionnant Enregistrer dans le ruban supérieur. \\\n",
    "Vous pouvez apporter d’autres modifications à ce rapport pour répondre à vos besoins en incluant ou en excluant d’autres tables ou colonnes.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-build-lakehouse/quick-summary-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481943e-9eb3-4493-8fb7-baec092e72d9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b163b-62f9-41e1-af5e-585a930c3390",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 6: Ingestion de données massive\n",
    "\n",
    "<mark>  Point important nous ne ferons pas l'import de donnéees complet afin d'eviter un traffic trop massif lors de l'import en parallèle de toutes les personnes présentent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8234891-4b1a-409e-8bed-8b800f55d2b4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Dans cette section, vous allez utiliser l’activité Copier des données du pipeline Data Factory pour ingérer des exemples de données d’un compte de stockage Azure vers la section Fichiers du lakehouse que vous avez créé précédemment.\n",
    "\n",
    "1.  Pour ce faire, se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>\n",
    "2. Une fois dans l'explorateur de lakehouse, dans le menu bandeau, se rendre sur \"Obtenir des données\" puis \"Nouveau pipeline de données\"\n",
    "3. Ensuite, configurez une connexion HTTP pour importer les exemples de données World Wide Importers dans le lakehouse. Dans la liste des Nouvelles sources, sélectionnez Afficher plus, recherchez HTTP et sélectionnez-le\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/select-http-connection.png)\n",
    "\n",
    "4. Dans la fenêtre Se connecter à la source de données, entrez les détails du tableau ci-dessous, puis sélectionnez Suivant.\n",
    "\n",
    "| Propriété | Valeur |\n",
    "| :--------- |:---------|\n",
    "| URL\t | https://assetsprod.microsoft.com/en-us/wwi-sample-dataset.zip |\n",
    "| Connexion | Créer une connexion |\n",
    "|Nom de la connexion |\twwisampledata |\n",
    "|Passerelle de données |\tAucun(e) |\n",
    "|Type d'authentification |\tAnonyme |\n",
    "\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/configure-http-connection.png)\n",
    "\n",
    "5. À l’étape suivante, activez la copie binaire et choisissez ZipDeflate (.zip) comme Type de compression, car la source est un fichier .zip. \\\n",
    "Conservez les valeurs par défaut des autres champs, puis cliquez sur Suivant.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/select-compression-type.png)\n",
    "\n",
    "6. Dans la fenêtre Se connecter à la destination des données, spécifiez Fichiers comme Dossier racine, puis sélectionnez Suivant. Cela écrit les données dans la section Fichiers du lakehouse.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/configure-destination-connection.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804630d8-3f82-4816-a924-d57e4ce8a038",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "7. Choisissez Binaire comme Format de fichier pour la destination. Sélectionnez Suivant, puis décocher la check box \"Démarrer le transfert de données immédiatement\", et validé avec \"OK\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375a316-2203-4207-bcd9-1f3db34b7fba",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d52561-90ce-4e0e-a5c4-dbc5abbcfed9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Etape 7: Création d'un shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96537f-18ab-4281-b8c5-04cbbbc00771",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Afin d'éviter l'import massif de données, vous allez mettre en place un shortcut vers le Onelake \"Fabric in a day\" \n",
    "\n",
    "Pour se faire se rendre dans votre workspace sur le lakehouse <mark>wwilakehouse</mark>. \\\n",
    "Dans l'explorateur dans le dossier files, faire un clic-droit, puis \"Nouveau raccourci\" \\\n",
    "Dans le choix des sources externes, choisir \"Microsoft Onelake\"  \\\n",
    "Choisir le lake nommée <mark>wwilakehouse</mark> qui est dans l'emplacement \"Claranet Fabric in a day\" \\\n",
    "Dans l'explorateur choisir \"Files\", puis \" wwi-raw-data\" et cocher le dossier \"wwi-raw-data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991258b0-f5c5-4048-8f2b-f402082aec74",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Vérifiez que le dossier WideWorldImportersDW est présent dans la vue Explorateur et contient des données pour toutes les tables.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-ingestion/validate-destination-files.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e1322-9afa-4d06-8b37-bbd07ea86e83",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7c671",
   "metadata": {},
   "source": [
    "## Etape 8: Préparer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854546f0",
   "metadata": {},
   "source": [
    "À partir des étapes précédentes du tutoriel, nous avons ingéré des données brutes de la source vers la section Fichiers du lakehouse. Vous pouvez maintenant transformer ces données et les préparer pour la création de tables Delta.\n",
    "\n",
    "Téléchargez les notebooks à partir du dossier https://github.com/microsoft/fabric-samples/tree/main/docs-samples/data-engineering/Lakehouse%20Tutorial%20Source%20Code \\\n",
    "Dans votre workspace, dans le bandeau sous le nom du workspace se rendre sur \"importer\", puis \"Bloc-notes\". \\\n",
    "Une fois l’importation réussie, accéder à la vue Éléments de l’espace de travail et voir les notebooks nouvellement importés. Sélectionnez le lakehouse wwilakehouse pour l’ouvrir. \n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/imported-notebooks-lakehouse.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69160b77",
   "metadata": {},
   "source": [
    "Une fois le lakehouse wwilakehouse ouvert, sélectionnez Ouvrir le notebook>existant dans le menu de navigation supérieur.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/existing-notebook-ribbon.png)\n",
    "\n",
    "Dans la liste des notebooks existants, sélectionnez le notebook 01 - Créer des tables Delta , puis sélectionnez Ouvrir. \\\n",
    "Dans le notebook ouvert dans l’explorateur lakehouse, vous voyez que le notebook est déjà lié à votre lakehouse ouvert. \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b1513",
   "metadata": {},
   "source": [
    "Avant d’écrire des données sous forme de tables Delta lake dans la section Tables du lakehouse, \\\n",
    "vous utilisez deux fonctionnalités Fabric (commande en V et Optimiser l’écriture) pour optimiser l’écriture des données et améliorer les performances de lecture. \\\n",
    "Pour activer ces fonctionnalités dans votre session, définissez ces configurations dans la première cellule de votre notebook. \n",
    "\n",
    "Pour démarrer le notebook et exécuter toutes les cellules dans l’ordre, sélectionnez Exécuter tout dans le ruban supérieur (sous Accueil). \\\n",
    "Ou, pour exécuter uniquement du code à partir d’une cellule spécifique, sélectionnez l’icône Exécuter qui apparaît à gauche de la cellule lors du pointage, \\\n",
    "ou appuyez sur MAJ + ENTRÉE sur votre clavier pendant que le contrôle se trouve dans la cellule. \n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/spark-session-run-execution.png)\n",
    "\n",
    "Lors de l’exécution d’une cellule, vous n’avez pas eu à spécifier les détails du pool ou du cluster Spark sous-jacents, car Fabric les fournit via Live pool. \\\n",
    "Chaque espace de travail Fabric est fourni avec un pool Spark par défaut, appelé Live Pool. \\\n",
    "Cela signifie que lorsque vous créez des notebooks, vous n’avez pas à vous soucier de spécifier des configurations Spark ou des détails de cluster. \\\n",
    "Lorsque vous exécutez la première commande de notebook, le pool dynamique est opérationnel en quelques secondes. \\\n",
    "Et la session Spark est établie et elle commence à exécuter le code. \\\n",
    "L’exécution du code suivante est presque instantanée dans ce notebook pendant que la session Spark est active. \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66c6ea",
   "metadata": {},
   "source": [
    "Ensuite, vous lisez les données brutes de la section Fichiers du lakehouse et ajoutez d’autres colonnes pour différentes parties de date dans le cadre de la transformation. \\\n",
    "Enfin, vous utilisez l’API partitionBy Spark pour partitionner les données avant de les écrire au format de table Delta en fonction des colonnes de partie de données nouvellement créées (Année et Trimestre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e68858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "table_name = 'fact_sale'\n",
    "\n",
    "df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/WideWorldImportersDW/parquet/full/fact_sale_1y_full')\n",
    "df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b53e0b",
   "metadata": {},
   "source": [
    "Une fois les tables de faits chargées, vous pouvez passer au chargement des données pour le reste des dimensions. \\\n",
    "La cellule suivante crée une fonction pour lire les données brutes de la section Fichiers du lakehouse pour chacun des noms de table passés en tant que paramètre. \\\n",
    "Ensuite, il crée une liste de tables de dimension. Enfin, il effectue une boucle dans la liste des tables et crée une table Delta pour chaque nom de table lu à partir du paramètre d’entrée. \\\n",
    "Notez que le script supprime la colonne nommée Photo dans cet exemple puisque la colonne n’est pas utilisée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def loadFullDataFromSource(table_name):\n",
    "    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/WideWorldImportersDW/parquet/full/' + table_name)\n",
    "    df = df.drop(\"Photo\")\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n",
    "\n",
    "full_tables = [\n",
    "    'dimension_city',\n",
    "    'dimension_customer',\n",
    "    'dimension_date',\n",
    "    'dimension_employee',\n",
    "    'dimension_stock_item'\n",
    "    ]\n",
    "\n",
    "for table in full_tables:\n",
    "    loadFullDataFromSource(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6adff6",
   "metadata": {},
   "source": [
    "Pour valider les tables créées, cliquez avec le bouton droit et sélectionnez Actualiser sur le lakehouse wwilakehouse. Les tables s’affichent.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/tutorial-lakehouse-explorer-tables.png)\n",
    "\n",
    "Revenez à la vue des éléments de l’espace de travail et sélectionnez le lakehouse wwilakehouse pour l’ouvrir. \\\n",
    "Maintenant, ouvrez le deuxième notebook. Dans la vue lakehouse, sélectionnez Ouvrir le notebook>existant dans le ruban. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857786f6",
   "metadata": {},
   "source": [
    "Dans la liste des notebooks existants, sélectionnez le notebook 02 - Transformation des données - Entreprise pour l’ouvrir.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/existing-second-notebook.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea127e43",
   "metadata": {},
   "source": [
    "Dans le notebook ouvert dans l’explorateur lakehouse, vous voyez que le notebook est déjà lié à votre lakehouse ouvert.\n",
    "\n",
    "Une organisation peut avoir des ingénieurs de données travaillant avec Scala/Python et d’autres ingénieurs données travaillant avec SQL (Spark SQL ou T-SQL), \\\n",
    "tous travaillant sur la même copie des données. Fabric permet à ces différents groupes, avec une expérience et des préférences variées, de travailler et de collaborer. \\\n",
    "Les deux approches différentes transforment et génèrent des agrégats métier. \\\n",
    "Vous pouvez choisir celle qui vous convient ou combiner ces approches en fonction de vos préférences sans compromettre les performances : \n",
    "\n",
    "- Approche 1 : Utilisez PySpark pour joindre et agréger des données pour générer des agrégats d’entreprise. Cette approche est préférable à une personne ayant un arrière-plan de programmation (Python ou PySpark).\n",
    "\n",
    "- Approche 2 : Utilisez Spark SQL pour joindre et agréger des données pour générer des agrégats d’entreprise. Cette approche est préférable à une personne ayant l’arrière-plan SQL, qui passe à Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea8eb9",
   "metadata": {},
   "source": [
    "#### Approche 1 (sale_by_date_city) :\n",
    "utilisez PySpark pour joindre et agréger des données pour générer des agrégats d’entreprise. \\\n",
    "Avec le code suivant, vous créez trois dataframes Spark différentes, chacune référençant une table Delta existante. \\\n",
    "Vous joignez ensuite ces tables à l’aide des cadres de données, vous effectuez des regroupements pour générer des agrégations, \\\n",
    "vous renommez quelques colonnes et vous écrivez enfin une table Delta dans la section Tables de la base de données pour qu’elle persiste avec les données. \\\n",
    "Dans cette cellule, vous créez trois dataframes Spark différentes, chacune référençant une table Delta existante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sale = spark.read.table(\"wwilakehouse.fact_sale\") \n",
    "df_dimension_date = spark.read.table(\"wwilakehouse.dimension_date\")\n",
    "df_dimension_city = spark.read.table(\"wwilakehouse.dimension_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106bcde",
   "metadata": {},
   "source": [
    "Ajoutez le code suivant à la même cellule pour joindre ces tables en utilisant les dataframes créées précédemment. \\\n",
    "Effectuez un regroupement « Group by » pour générer une agrégation, renommer quelques colonnes et la écrire enfin en tant que table Delta dans la section Tables du lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_city = df_fact_sale.alias(\"sale\") \\\n",
    ".join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\") \\\n",
    ".join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\") \\\n",
    ".select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
    ".groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\\\n",
    ".sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
    ".withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\\\n",
    ".withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\\\n",
    ".withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\\\n",
    ".withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\\\n",
    ".orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
    "\n",
    "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165ff7b",
   "metadata": {},
   "source": [
    "#### Approche 2 (sale_by_date_employee) : \n",
    "utilisez Spark SQL pour joindre et agréger des données pour générer des agrégats d’entreprise. \\\n",
    "Avec le code suivant, vous créez une vue Spark temporaire en joignant trois tables, en effectuant un Regrouper par pour générer une agrégation et en renommant quelques colonnes. \\\n",
    "Enfin, vous lisez à partir de la vue Spark temporaire et vous l’écrivez enfin sous la forme d’une table Delta dans la section Tables du lakehouse pour conserver les données.\n",
    "\n",
    "Dans cette cellule, vous créez une vue Spark temporaire en joignant trois tables, effectuez un Regrouper par pour générer l’agrégation et renommez quelques-unes des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW sale_by_date_employee\n",
    "AS\n",
    "SELECT\n",
    "       DD.Date, DD.CalendarMonthLabel\n",
    " , DD.Day, DD.ShortMonth Month, CalendarYear Year\n",
    "      ,DE.PreferredName, DE.Employee\n",
    "      ,SUM(FS.TotalExcludingTax) SumOfTotalExcludingTax\n",
    "      ,SUM(FS.TaxAmount) SumOfTaxAmount\n",
    "      ,SUM(FS.TotalIncludingTax) SumOfTotalIncludingTax\n",
    "      ,SUM(Profit) SumOfProfit \n",
    "FROM wwilakehouse.fact_sale FS\n",
    "INNER JOIN wwilakehouse.dimension_date DD ON FS.InvoiceDateKey = DD.Date\n",
    "INNER JOIN wwilakehouse.dimension_Employee DE ON FS.SalespersonKey = DE.EmployeeKey\n",
    "GROUP BY DD.Date, DD.CalendarMonthLabel, DD.Day, DD.ShortMonth, DD.CalendarYear, DE.PreferredName, DE.Employee\n",
    "ORDER BY DD.Date ASC, DE.PreferredName ASC, DE.Employee ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c65ea",
   "metadata": {},
   "source": [
    "Dans cette cellule, vous lisez à partir de la vue Spark temporaire créée dans la cellule précédente, puis vous l’écrivez en tant que table Delta dans la section Tables du lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e993cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_employee = spark.sql(\"SELECT * FROM sale_by_date_employee\")\n",
    "sale_by_date_employee.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaff6d5",
   "metadata": {},
   "source": [
    "Pour valider les tables créées, cliquez avec le bouton droit et sélectionnez Actualiser sur le lakehouse wwilakehouse. \\\n",
    "Les tables d’agrégation s’affichent.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-data-preparation/validate-tables.png)\n",
    "\n",
    "Les deux approches produisent un résultat similaire. Afin de réduire le besoin d’apprendre une nouvelle technologie ou de compromettre les performances, \\\n",
    "choisissez l’approche qui correspond au mieux à votre arrière-plan et à vos préférences,\n",
    "\n",
    "Vous pouvez remarquer que vous écrivez des données sous forme de fichiers Delta lake. \\\n",
    "La fonctionnalité automatique de découverte et d’inscription de tables de Fabric les récupère et les inscrit dans le metastore. \n",
    "\n",
    "Vous n’avez pas besoin d’appeler CREATE TABLE explicitement des instructions pour créer des tables à utiliser avec SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe31a1e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c2496",
   "metadata": {},
   "source": [
    "## Etape 9 : Création d'un rapport manuel sur les données du lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844dc8b",
   "metadata": {},
   "source": [
    "Power BI est intégré en mode natif à l’ensemble de l’expérience Fabric. \\\n",
    "Cette intégration native inclut un mode unique, appelé DirectLake, \\\n",
    "afin d’accéder aux données à partir du lakehouse pour fournir l’expérience de requête et de création de rapports la plus performante. \\\n",
    "DirectLake est une nouvelle fonctionnalité révolutionnaire vous permettant d’analyser des modèles sémantiques très volumineux dans Power BI. \\\n",
    "Avec Direct Lake, vous chargez de fichiers au format Parquet directement à partir d’un lac de données sans avoir à interroger un point de terminaison lakehouse ou un Data Warehouse, \\\n",
    "et sans avoir à importer ou à dupliquer des données dans un modèle sémantique Power BI. \\\n",
    "Direct Lake est un chemin d’accès rapide pour charger les données depuis le lac directement dans le moteur Power BI, prêtes pour l’analyse.\n",
    "\n",
    "En mode DirectQuery traditionnel, le moteur Power BI interroge directement les données de la source pour chaque exécution de requête, \\\n",
    "et les performances des requêtes dépendent de la vitesse de récupération des données. DirectQuery élimine la nécessité de copier des données, \\\n",
    "garantissant ainsi que toutes les modifications apportées à la source sont immédiatement répercutées dans les résultats de la requête. \\\n",
    "En revanche, en mode importation, les performances sont meilleures, \\\n",
    "car les données sont facilement disponibles en mémoire sans avoir à interroger les données à partir de la source pour chaque exécution de requête. \\\n",
    "Toutefois, le moteur Power BI doit d’abord copier les données dans la mémoire au moment de l’actualisation des données. \\\n",
    "Toutes les modifications apportées à la source de données sous-jacente sont récupérées lors de la prochaine actualisation des données (dans la planification et l’actualisation à la demande).\n",
    "\n",
    "Le mode DirectLake élimine désormais cette exigence d’importation en chargeant les fichiers de données directement en mémoire. \\\n",
    "Comme il n'y a pas de processus d'importation explicite, il est possible d'intégrer les changements à la source au fur et à mesure qu'ils se produisent, \\\n",
    "combinant ainsi les avantages de DirectQuery et du mode d'importation tout en évitant leurs inconvénients. \\\n",
    "Le mode DirectLake est le choix idéal pour l’analyse de modèles sémantiques très volumineux et de modèles sémantiques avec des mises à jour fréquentes à la source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87075f18",
   "metadata": {},
   "source": [
    "1. Dans votre lakehouse wwilakehouse, sélectionnez point de terminaison d’analytique SQL dans le menu déroulant Lakehouse en haut à droite de l’écran.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/load-data-choose-sql-endpoint.png)\n",
    "\n",
    "2. Dans le volet point de terminaison d'analytique SQL, vous devez être en mesure de voir toutes les tables que vous avez créées. Si vous ne les voyez pas encore, sélectionnez l’icône Actualiser en haut. Ensuite, sélectionnez l’onglet Modèle dans le bandeau pour ouvrir le modèle sémantique Power BI par défaut.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/warehouse-mode-refresh-model.png)\n",
    "\n",
    "3. Pour ce modèle de données, vous devez définir la relation entre différentes tables afin de pouvoir créer des rapports et des visualisations basés sur les données provenant de différentes tables. À partir de la table fact_sale , faites glisser le champ CityKey et déposez-le dans le champ CityKey de la table dimension_city pour créer une relation. La boîte de dialogue Nouvelle relation apparaît.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/drag-drop-tables-relationships.png)\n",
    "\n",
    "4. Dans la boîte de dialogue Nouvelle relation :\n",
    "- Le tableau 1 est rempli avec fact_sale et la colonne CityKey.\n",
    "- Le tableau 2 est rempli avec dimension_city et la colonne CityKey.\n",
    "- Cardinalité : Plusieurs-à-un (*:1).\n",
    "- Direction du filtrage croisé : A sens unique.\n",
    "- Laissez la case en regard de Rendre cette relation active sélectionnée.\n",
    "- Cochez la case en regard de Supposer l’intégrité référentielle.\n",
    "- Cliquez sur Enregistrer.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/create-relationship-dialog.png)\n",
    "\n",
    "5. Ensuite, ajoutez ces relations avec les mêmes paramètres Nouvelle relation que ceux indiqués à l’étape précédente, mais avec les tables et colonnes suivantes :\n",
    "- StockItemKey(fact_sale) - StockItemKey(dimension_stock_item)\n",
    "- Salespersonkey(fact_sale) - EmployeeKey(dimension_employee)\n",
    "- CustomerKey(fact_sale) - CustomerKey(dimension_customer)\n",
    "- InvoiceDateKey(fact_sale) - Date(dimension_date)\n",
    "\n",
    "Après avoir ajouté ces relations, votre modèle de données est prêt pour la création de rapports, comme illustré dans l’image suivante :\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/new-report-relationships.png)\n",
    "\n",
    "6. Sélectionnez Nouveau rapport pour commencer à créer des rapports/tableaux de bord dans Power BI. Sur le canevas de rapport Power BI, vous pouvez créer des rapports pour répondre à vos besoins métier en faisant glisser les colonnes requises du volet Données vers le canevas et en utilisant une ou plusieurs visualisations disponibles.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/report-canvas-drag-columns.png)\n",
    "\n",
    "7. Ajouter un titre :\n",
    "\n",
    "Dans le ruban, sélectionnez Zone de texte.\n",
    "\n",
    "Entrez WW Importers Profit Reporting.\n",
    "\n",
    "Mettez en surbrillance le texte, augmentez la taille à 20 et déplacez-le dans le coin supérieur gauche de la page de rapport.\n",
    "\n",
    "8. Ajouter une carte :\n",
    "\n",
    "Dans le volet Données, développez fact_sale et cochez la case en regard de Profit. Cette sélection crée un histogramme et ajoute le champ à l’axe Y.\n",
    "\n",
    "Une fois le graphique sélectionné, sélectionnez le visuel Carte dans le volet de visualisation. Cette sélection convertit le visuel en carte.\n",
    "\n",
    "Placez la carte sous le titre.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/card-visualization.png)\n",
    "\n",
    "9. Ajouter un graphique à barres :\n",
    "\n",
    "Dans le volet Données, développez fact_sales et cochez la case en regard de Profit. Cette sélection crée un histogramme et ajoute le champ à l’axe des abscisses.\n",
    "\n",
    "Dans le volet Données, développez dimension_city et cochez la case SalesTerritory. Cette sélection ajoute le champ à l’axe Y.\n",
    "\n",
    "Une fois le graphique à barres sélectionné, sélectionnez le visuel Graphique à barres cluster dans le volet de visualisation. Cette sélection convertit l’histogramme en graphique à barres.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/build-visual-bar-chart.png)\n",
    "\n",
    "Redimensionnez le graphique à barres et déplacez-le sous le titre et la carte.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/resize-bar-chart-under-card.png)\n",
    "\n",
    "10. Cliquez n’importe où sur le canevas vide (ou appuyez sur la touche Échap) pour désélectionner le graphique à barres.\n",
    "\n",
    "11. Créez un visuel de graphique en aires empilées :\n",
    "\n",
    "Dans le volet Visualisations , sélectionnez le visuel Graphique en aires empilées.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/stacked-area-chart.png)\n",
    "\n",
    "Repositionnez et redimensionnez le graphique en aires empilées à droite des visuels carte et à barres créés au cours des étapes précédentes.\n",
    "\n",
    "Dans le volet Données, développez fact_sales et cochez la case en regard de Profit. Développez dimension_date et cochez la case en regard de FiscalMonthNumber. Cette sélection crée un graphique en courbes rempli montrant les bénéfices par mois fiscal.\n",
    "\n",
    "Dans le volet Données, développez dimension_stock_item et faites glisser BuyingPackage dans le champ Légende. Cette sélection ajoute une ligne pour chacun des packages d’achat.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/data-pane-change-chart.png)\n",
    "\n",
    "12. Cliquez n’importe où sur le canevas vide (ou appuyez sur la touche Échap) pour désélectionner le graphique en aires empilées.\n",
    "\n",
    "13. Créer un histogramme :\n",
    "\n",
    "Dans le volet Visualisations , sélectionnez le visuel Histogramme empilé.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/select-stacked-column-chart.png)\n",
    "\n",
    "Dans le volet Données, développez fact_sales et cochez la case en regard de Profit. Cette sélection ajoute le champ à l’axe Y.\n",
    "\n",
    "Dans le volet Données, développez dimension_employee et cochez la case en regard de Employé. Cette sélection ajoute le champ à l’axe X.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-engineering/media/tutorial-lakehouse-build-report/add-field-x-axis.png)\n",
    "\n",
    "14. Cliquez n’importe où sur le canevas vide (ou appuyez sur la touche Échap) pour désélectionner le graphique.\n",
    "\n",
    "15. Dans le ruban, sélectionnez Fichier>Enregistrer.\n",
    "\n",
    "16. Entrez le nom de votre rapport en tant que Rapport sur les bénéfices.\n",
    "\n",
    "17. Sélectionnez Enregistrer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13015267",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291856a",
   "metadata": {},
   "source": [
    "## Etape 10:  notebook et Data Wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694f927",
   "metadata": {},
   "source": [
    "Dans ce tutoriel, vous allez apprendre à effectuer une analyse exploratoire des données (EDA) pour examiner et examiner les données tout en récapitulé ses principales caractéristiques à l’aide des techniques de visualisation des données.\n",
    "\n",
    "Vous utiliserez seaborn, une bibliothèque de visualisation de données Python qui fournit une interface de haut niveau pour créer des visuels sur des dataframes et des tableaux. Pour plus d’informations sur seaborn, consultez Seaborn : [Statistical Data Visualisation](https://seaborn.pydata.org/).\n",
    "\n",
    "Vous utiliserez également [data Wrangler](https://learn.microsoft.com/fr-fr/fabric/data-science/data-wrangler), un outil basé sur un notebook qui vous offre une expérience immersive pour effectuer une analyse exploratoire des données et le nettoyage.\n",
    "\n",
    "Les principales étapes de ce didacticiel sont les suivantes :\n",
    "\n",
    "Lire les données stockées à partir d’une table delta dans le lakehouse.\n",
    "Convertissez un DataFrame Spark en DataFrame Pandas, pris en charge par les bibliothèques de visualisation Python.\n",
    "Utilisez Data Wrangler pour effectuer le nettoyage et la transformation initiaux des données.\n",
    "Effectuez une analyse exploratoire des données à l’aide de seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290b623",
   "metadata": {},
   "source": [
    "Le jeu de données contient l’état d’attrition de 10 000 clients. Il inclut également des attributs qui peuvent avoir un impact sur la perte de clientèle, par exemple :\n",
    "\n",
    "Score de crédit \\\n",
    "Emplacement géographique (Allemagne, France, Espagne) \\\n",
    "Sexe (mâle, femelle) \\\n",
    "Âge \\\n",
    "Mandat (années de client de la banque) \\\n",
    "Solde du compte \\\n",
    "Salaire estimé \\\n",
    "Nombre de produits qu’un client a achetés par le biais de la banque \\\n",
    "État de la carte de crédit (qu’un client dispose d’une carte de crédit ou non) \\\n",
    "État du membre actif (qu’il s’agisse du client d’une banque active ou non) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5d8a5",
   "metadata": {},
   "source": [
    "Dans votre workspace ouvrir le lakehouse, et dans le menu bandeau \"Ouvrir le notebook\" -> \"Nouveau notebook\" \\\n",
    "Pour cette exercice vous aller devoir créer votre notebook en reprenant les blocs de codes. \\\n",
    "ensuite créer un premier bloc de code python avec le contenu suivant et l'executer pour initialiser l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56728139",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CUSTOM_DATA = False  # if TRUE, dataset has to be uploaded manually\n",
    "\n",
    "DATA_ROOT = \"/lakehouse/default\"\n",
    "DATA_FOLDER = \"Files/churn\"  # folder with data files\n",
    "DATA_FILE = \"churn.csv\"  # data file name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc638",
   "metadata": {},
   "source": [
    "Ensuite executer le bloc suivant, ce code télécharge une version publique du jeu de données, puis la stocke dans un lakehouse Fabric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "if not IS_CUSTOM_DATA:\n",
    "# Download demo data files into lakehouse if not exist\n",
    "    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn\"\n",
    "    file_list = [DATA_FILE]\n",
    "    download_path = f\"{DATA_ROOT}/{DATA_FOLDER}/raw\"\n",
    "\n",
    "    if not os.path.exists(\"/lakehouse/default\"):\n",
    "        raise FileNotFoundError(\n",
    "            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n",
    "        )\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    for fname in file_list:\n",
    "        if not os.path.exists(f\"{download_path}/{fname}\"):\n",
    "            r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n",
    "            with open(f\"{download_path}/{fname}\", \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "    print(\"Downloaded demo data files into lakehouse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2aeeb",
   "metadata": {},
   "source": [
    "Lecture des données du fichier télécharger précedement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"Files/churn/raw/churn.csv\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a62e5",
   "metadata": {},
   "source": [
    "Convertissez le DataFrame spark en DataFrame pandas pour faciliter le traitement et la visualisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0df0",
   "metadata": {},
   "source": [
    "Explorez les données brutes avec display, effectuez des statistiques de base et affichez des vues de graphique. \\\n",
    "Notez que vous devez d’abord importer les bibliothèques requises telles que Numpy, Pandas, Seabornet Matplotlib pour l’analyse et la visualisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc860d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import rc, rcParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e40964",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8503aa",
   "metadata": {},
   "source": [
    "Pour explorer et transformer tout DataFrame Pandas de votre notebook, lancez Data Wrangler directement à partir de ce dernier. \n",
    "1. Sous l’onglet Données du ruban du notebook, sélectionnez Lancer Data Wrangler. Vous verrez la liste des DataFrames pandas activés disponibles pour modification.\n",
    "2. Sélectionnez le DataFrame que vous souhaitez ouvrir dans Data Wrangler. Étant donné que ce notebook ne contient qu’un seul DataFrame, df, sélectionnez df.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/launch-data-wrangler.png)\n",
    "\n",
    "Data Wrangler lance et génère une vue d’ensemble descriptive de vos données. La table au milieu affiche chaque colonne de données. Le panneau Résumé en regard du tableau affiche des informations sur le DataFrame. Lorsque vous sélectionnez une colonne dans le tableau, le résumé est mis à jour avec des informations sur la colonne sélectionnée. Dans certains cas, les données affichées et résumées sont une vue tronquée de votre DataFrame. Lorsque cela se produit, vous verrez l’image d’avertissement dans le volet résumé. Pointez sur cet avertissement pour afficher le texte expliquant la situation.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/preview-inline.png)\n",
    "\n",
    "Chaque opération que vous effectuez peut être appliquée en quelques clics, en mettant à jour l’affichage des données en temps réel et en générant du code que vous pouvez enregistrer dans votre bloc-notes en tant que fonction réutilisable.\n",
    "\n",
    "Le reste de cette section vous guide tout au long des étapes de nettoyage des données avec Data Wrangler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7050a1",
   "metadata": {},
   "source": [
    "Dans le volet gauche, vous trouverez une liste d’opérations (telles que Rechercher et remplacer, Format, Formules, Numérique) que vous pouvez effectuer sur le jeu de données.\n",
    "\n",
    "1. Développez Rechercher et remplacer, puis sélectionnez Supprimer les lignes dupliquées.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/expand-section.png)\n",
    "\n",
    "2. Un panneau s’affiche pour vous permettre de sélectionner la liste des colonnes que vous souhaitez comparer pour définir une ligne en double. Sélectionnez RowNumber et CustomerId.\n",
    "\n",
    "Dans le panneau central, vous trouverez un aperçu des résultats de cette opération. Sous l'aperçu se trouve le code pour effectuer l'opération. \\\n",
    "Dans cette instance, les données semblent inchangées. Mais étant donné que vous examinez une vue tronquée, il est judicieux de néanmoins appliquer l’opération.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/drop-duplicate-inline.png)\n",
    "\n",
    "3. Sélectionnez Appliquer (à côté ou en bas) pour passer à l’étape suivante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432edc3",
   "metadata": {},
   "source": [
    "Utilisez Data Wrangler pour supprimer des lignes avec des données manquantes sur toutes les colonnes.\n",
    "\n",
    "1. Sélectionnez Supprimer les valeurs manquantes dans Rechercher et remplacer.\n",
    "\n",
    "2. Choisissez Tout sélectionner dans les Colonnes cibles.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/drop-missing-inline.png)\n",
    "\n",
    "3. Sélectionnez Appliquer pour passer à l’étape suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e342c21",
   "metadata": {},
   "source": [
    "Utilisez Data Wrangler pour supprimer des colonnes dont vous n’avez pas besoin.\n",
    "\n",
    "1. Développez le schéma , puis sélectionnez Supprimer les colonnes.\n",
    "\n",
    "2. Sélectionnez Numéro de ligne, Identifiant client, Nom de famille. Ces colonnes apparaissent en rouge dans l’aperçu pour indiquer qu’elles sont modifiées par le code (dans ce cas, supprimées.)\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/drop-columns-inline.png)\n",
    "\n",
    "3. Sélectionnez Appliquer pour passer à l’étape suivante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d48066",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694eaa9",
   "metadata": {},
   "source": [
    "## Etape 11:  Exploration via notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9f9e3",
   "metadata": {},
   "source": [
    "Utilisez ce code pour déterminer les attributs catégoriels, numériques et cibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a693c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dependent (target) attribute\n",
    "dependent_variable_name = \"Exited\"\n",
    "print(dependent_variable_name)\n",
    "# Determine the categorical attributes\n",
    "categorical_variables = [col for col in df_clean.columns if col in \"O\"\n",
    "                        or df_clean[col].nunique() <=5\n",
    "                        and col not in \"Exited\"]\n",
    "print(categorical_variables)\n",
    "# Determine the numerical attributes\n",
    "numeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != \"object\"\n",
    "                        and df_clean[col].nunique() >5]\n",
    "print(numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63749e8b",
   "metadata": {},
   "source": [
    "Afficher le résumé à cinq nombres (score minimal, premier quartile, médiane, troisième quartile, score maximal) pour les attributs numériques, à l’aide de tracés de zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_cols = df_clean[numeric_variables]\n",
    "sns.set(font_scale = 0.7) \n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw =  dict(hspace=0.3), figsize = (17,8))\n",
    "fig.tight_layout()\n",
    "for ax,col in zip(axes.flatten(), df_num_cols.columns):\n",
    "    sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n",
    "fig.delaxes(axes[1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644d51b",
   "metadata": {},
   "source": [
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/box-plots.jpg)\n",
    "\n",
    "Affichez la distribution des clients sortants et non sortants selon les attributs catégoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd38a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure']\n",
    "fig, axarr = plt.subplots(2, 3, figsize=(15, 4))\n",
    "for ind, item in enumerate (attr_list):\n",
    "    sns.countplot(x = item, hue = 'Exited', data = df_clean, ax = axarr[ind%2][ind//2])\n",
    "fig.subplots_adjust(hspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862dd74",
   "metadata": {},
   "source": [
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/bar-charts.jpg)\n",
    "\n",
    "Afficher la distribution de fréquence d’attributs numériques à l’aide de l’histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c577f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_num_cols.columns[: len(df_num_cols.columns)]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18, 8)\n",
    "length = len(columns)\n",
    "for i,j in itertools.zip_longest(columns, range(length)):\n",
    "    plt.subplot((length // 2), 3, j+1)\n",
    "    plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n",
    "    df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b316af2",
   "metadata": {},
   "source": [
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-explore-notebook/histograms.jpg)\n",
    "\n",
    "Effectuez l’ingénierie des fonctionnalités pour générer de nouveaux attributs en fonction des attributs actuels :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07894e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"NewTenure\"] = df_clean[\"Tenure\"]/df_clean[\"Age\"]\n",
    "df_clean[\"NewCreditsScore\"] = pd.qcut(df_clean['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\n",
    "df_clean[\"NewAgeScore\"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_clean[\"NewBalanceScore\"] = pd.qcut(df_clean['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n",
    "df_clean[\"NewEstSalaryScore\"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f40b3b",
   "metadata": {},
   "source": [
    "Résumé des observations de l’analyse exploratoire des données\n",
    "- La plupart des clients proviennent de la France par rapport à l’Espagne et à l’Allemagne, tandis que l’Espagne a le taux d’évolution le plus bas comparé à la France et à l’Allemagne.\n",
    "- La plupart des clients ont des cartes de crédit.\n",
    "- Il existe des clients dont le score d’âge et de crédit est supérieur à 60 et inférieur à 400, respectivement, mais ils ne peuvent pas être considérés comme des valeurs hors norme.\n",
    "- Très peu de clients ont plus de deux des produits de la banque.\n",
    "- Les clients qui ne sont pas actifs ont un taux d’attrition plus élevé.\n",
    "- Les années d'ancienneté et le genre ne semblent pas avoir d'impact sur la décision du client de fermer le compte bancaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708253e5",
   "metadata": {},
   "source": [
    "Nettoyage des données pour l'exercice suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "def clean_data(df_clean):\n",
    "    # One-hot encode columns: 'Geography', 'Gender'\n",
    "    df_clean = pd.get_dummies(df_clean, columns=['Geography', 'Gender'])\n",
    "    return df_clean\n",
    " \n",
    "df_clean_1 = clean_data(df_clean.copy())\n",
    "df_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944126c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"df_clean\"\n",
    "# Create Spark DataFrame from pandas\n",
    "sparkDF=spark.createDataFrame(df_clean_1) \n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark dataframe saved to delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801af49",
   "metadata": {},
   "source": [
    "## Etape 12: Entrainement de modèle de machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c181e",
   "metadata": {},
   "source": [
    "Pour ce notebook, vous installerez imbalanced-learn (importé en tant que imblearn) à l’aide de %pip install. \\\n",
    "imbalanced-learn est une bibliothèque pour la technique SMOTE (Synthetic Minority Oversampling Technique), utilisée pour traiter les jeux de données déséquilibrés. \\\n",
    "Le noyau PySpark est redémarré après %pip install. Par conséquent, nous devons installer la bibliothèque avant d’exécuter d’autres cellules.\n",
    "\n",
    "Vous accéderez à SMOTE à l’aide de la bibliothèque imblearn. Installez-la maintenant à l’aide des fonctionnalités d’installation en ligne (par exemple %pip, %conda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imblearn for SMOTE using pip\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c73d61",
   "metadata": {},
   "source": [
    "Lorsque vous installez une bibliothèque dans un notebook, elle est disponible uniquement pendant la durée de la session de notebook et non dans l’espace de travail. Si vous redémarrez le notebook, vous devez réinstaller la bibliothèque.\n",
    "\n",
    "\n",
    "Avant d’entraîner un modèle Machine Learning, vous devez charger la table delta à partir du lakehouse afin de lire les données nettoyées que vous avez créées dans le notebook précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5fa262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "SEED = 12345\n",
    "df_clean = spark.read.format(\"delta\").load(\"Tables/df_clean\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d247352",
   "metadata": {},
   "source": [
    "Cette section montre comment générer une expérience, spécifier les paramètres de modèles Machine Learning et d’entraînement ainsi que les métriques de scoring, \\\n",
    "entraîner les modèles Machine Learning, les journaliser et enregistrer les modèles ainsi entraînés pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Setup experiment name\n",
    "EXPERIMENT_NAME = \"bank-churn-experiment\"  # MLflow experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11274927",
   "metadata": {},
   "source": [
    "L’extension des fonctionnalités de synchronisation automatique MLflow fonctionne en capturant automatiquement les valeurs des paramètres d’entrée et des métriques de sortie d’un modèle Machine Learning tel qu’il est entraîné. \\\n",
    "Ces informations sont ensuite consignées dans votre espace de travail, où elles sont accessibles et visualisées à l’aide des API MLflow ou de l’expérience correspondante dans votre espace de travail.\n",
    "\n",
    "Toutes les expériences avec leurs noms respectifs sont journalisées et vous serez en mesure de suivre leurs paramètres et métriques de performances. Pour en savoir plus sur l’autologging, consultez [Autologging dans Microsoft Fabric](https://aka.ms/fabric-autologging).\n",
    "\n",
    "Activation de l'autologging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae05bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.autolog(exclusive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f40e78",
   "metadata": {},
   "source": [
    "Avec vos données en place, vous pouvez maintenant définir les modèles Machine Learning. \\\n",
    "Vous allez appliquer des modèles de forêt aléatoire et LightGBM dans ce notebook. \\\n",
    "Utilisez scikit-learn et lightgbm pour implémenter les modèles dans quelques lignes de code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bec160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02767349",
   "metadata": {},
   "source": [
    "Utilisez la fonction train_test_split de scikit-learn pour fractionner les données en jeux de données d’entraînement, de validation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean[\"Exited\"]\n",
    "X = df_clean.drop(\"Exited\",axis=1)\n",
    "# Split the dataset to 60%, 20%, 20% for training, validation, and test datasets\n",
    "# Train-Test Separation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n",
    "# Train-Validation Separation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f0157",
   "metadata": {},
   "source": [
    "Enregistrez les données de test dans la table delta en vue de les utiliser dans le notebook suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09857093",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"df_test\"\n",
    "# Create PySpark DataFrame from Pandas\n",
    "df_test=spark.createDataFrame(X_test)\n",
    "df_test.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark test DataFrame saved to delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f75e07",
   "metadata": {},
   "source": [
    "L’exploration des données dans la partie 2 a montré que sur les 10 000 points de données correspondant à 10 000 clients, \\\n",
    "seuls 2 037 clients (environ 20 %) ont quitté la banque. Cela signifie que le jeu de données est très déséquilibré. \\\n",
    "Le problème avec la classification déséquilibrée est qu’il y a trop peu d’exemples de la classe minoritaire pour qu’un modèle apprenne efficacement la limite de décision. \\\n",
    "La SMOTE est l’approche la plus largement utilisée pour synthétiser de nouveaux échantillons pour la classe minoritaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=SEED)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "new_train = pd.concat([X_res, y_res], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279184af",
   "metadata": {},
   "source": [
    "Entraîner le modèle à l’aide de la forêt d’arbres décisionnels avec une profondeur maximale de 4 et 4 caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(registered_model_name='rfc1_sm') # Register the trained model with autologging\n",
    "rfc1_sm = RandomForestClassifier(max_depth=4, max_features=4, min_samples_split=3, random_state=1) # Pass hyperparameters\n",
    "with mlflow.start_run(run_name=\"rfc1_sm\") as run:\n",
    "    rfc1_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n",
    "    print(\"run_id: {}; status: {}\".format(rfc1_sm_run_id, run.info.status))\n",
    "    # rfc1.fit(X_train,y_train) # Imbalanaced training data\n",
    "    rfc1_sm.fit(X_res, y_res.ravel()) # Balanced training data\n",
    "    rfc1_sm.score(X_val, y_val)\n",
    "    y_pred = rfc1_sm.predict(X_val)\n",
    "    cr_rfc1_sm = classification_report(y_val, y_pred)\n",
    "    cm_rfc1_sm = confusion_matrix(y_val, y_pred)\n",
    "    roc_auc_rfc1_sm = roc_auc_score(y_res, rfc1_sm.predict_proba(X_res)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225a533",
   "metadata": {},
   "source": [
    "Entraîner le modèle à l’aide de la forêt d’arbres décisionnels avec une profondeur maximale de 8 et 6 caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af34a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(registered_model_name='rfc2_sm') # Register the trained model with autologging\n",
    "rfc2_sm = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=3, random_state=1) # Pass hyperparameters\n",
    "with mlflow.start_run(run_name=\"rfc2_sm\") as run:\n",
    "    rfc2_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n",
    "    print(\"run_id: {}; status: {}\".format(rfc2_sm_run_id, run.info.status))\n",
    "    # rfc2.fit(X_train,y_train) # Imbalanced training data\n",
    "    rfc2_sm.fit(X_res, y_res.ravel()) # Balanced training data\n",
    "    rfc2_sm.score(X_val, y_val)\n",
    "    y_pred = rfc2_sm.predict(X_val)\n",
    "    cr_rfc2_sm = classification_report(y_val, y_pred)\n",
    "    cm_rfc2_sm = confusion_matrix(y_val, y_pred)\n",
    "    roc_auc_rfc2_sm = roc_auc_score(y_res, rfc2_sm.predict_proba(X_res)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b4a8f",
   "metadata": {},
   "source": [
    "Entraîner le modèle à l’aide de LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_model\n",
    "mlflow.lightgbm.autolog(registered_model_name='lgbm_sm') # Register the trained model with autologging\n",
    "lgbm_sm_model = LGBMClassifier(learning_rate = 0.07, \n",
    "                        max_delta_step = 2, \n",
    "                        n_estimators = 100,\n",
    "                        max_depth = 10, \n",
    "                        eval_metric = \"logloss\", \n",
    "                        objective='binary', \n",
    "                        random_state=42)\n",
    "\n",
    "with mlflow.start_run(run_name=\"lgbm_sm\") as run:\n",
    "    lgbm1_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n",
    "    # lgbm_sm_model.fit(X_train,y_train) # Imbalanced training data\n",
    "    lgbm_sm_model.fit(X_res, y_res.ravel()) # Balanced training data\n",
    "    y_pred = lgbm_sm_model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    cr_lgbm_sm = classification_report(y_val, y_pred)\n",
    "    cm_lgbm_sm = confusion_matrix(y_val, y_pred)\n",
    "    roc_auc_lgbm_sm = roc_auc_score(y_res, lgbm_sm_model.predict_proba(X_res)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9ff1c",
   "metadata": {},
   "source": [
    "Les exécutions de l’expérience sont automatiquement enregistrées dans l’artefact d’expérience qui peut être trouvé à partir de l’espace de travail. Elles sont nommées en fonction du nom utilisé pour définir l’expérience. Tous les modèles Machine Learning entraînés sont journalisés, de même que leurs exécutions, leurs métriques de performances et leurs paramètres de modèles.\n",
    "\n",
    "Pour afficher vos expériences :\n",
    "\n",
    "1. Dans le volet gauche, sélectionnez votre espace de travail.\n",
    "\n",
    "2. En haut à droite, filtrez pour afficher uniquement les expériences, afin de faciliter la recherche de celle que vous recherchez.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-train-models/filter-workspace.png)\n",
    "\n",
    "3. Recherchez et sélectionnez le nom de l’expérience. Dans ce cas bank-churn-experiment. Si l’expérience n’apparaît pas dans votre espace de travail, actualisez votre navigateur.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-train-models/experiment-runs.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695beb3",
   "metadata": {},
   "source": [
    "Une fois l’apprentissage de modèles Machine Learning terminé, vous pouvez évaluer les performances des modèles entraînés de deux manières.\n",
    "\n",
    "Ouvrez l’expérience enregistrée à partir de l’espace de travail, chargez les modèles Machine Learning, \\\n",
    "puis évaluez les performances des modèles ainsi chargés sur le jeu de données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define run_uri to fetch the model\n",
    "# mlflow client: mlflow.model.url, list model\n",
    "load_model_rfc1_sm = mlflow.sklearn.load_model(f\"runs:/{rfc1_sm_run_id}/model\")\n",
    "load_model_rfc2_sm = mlflow.sklearn.load_model(f\"runs:/{rfc2_sm_run_id}/model\")\n",
    "load_model_lgbm1_sm = mlflow.lightgbm.load_model(f\"runs:/{lgbm1_sm_run_id}/model\")\n",
    "# Assess the performance of the loaded model on validation dataset\n",
    "ypred_rfc1_sm_v1 = load_model_rfc1_sm.predict(X_val) # Random Forest with max depth of 4 and 4 features\n",
    "ypred_rfc2_sm_v1 = load_model_rfc2_sm.predict(X_val) # Random Forest with max depth of 8 and 6 features\n",
    "ypred_lgbm1_sm_v1 = load_model_lgbm1_sm.predict(X_val) # LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc48f1c",
   "metadata": {},
   "source": [
    "Évaluez directement les performances des modèles Machine Learning entraînés sur le jeu de données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87178e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_rfc1_sm_v2 = rfc1_sm.predict(X_val) # Random Forest with max depth of 4 and 4 features\n",
    "ypred_rfc2_sm_v2 = rfc2_sm.predict(X_val) # Random Forest with max depth of 8 and 6 features\n",
    "ypred_lgbm1_sm_v2 = lgbm_sm_model.predict(X_val) # LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb992c",
   "metadata": {},
   "source": [
    "Selon vos préférences, l’une ou l’autre approche est correcte et doit fournir des performances identiques. Dans ce notebook, vous allez choisir la première approche afin de mieux illustrer les fonctionnalités de journalisation automatique MLflow dans Microsoft Fabric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d43c2f",
   "metadata": {},
   "source": [
    "Ensuite, vous développerez un script pour tracer la matrice de confusion afin d’évaluer la précision de la classification à l’aide du jeu de données de validation. \\\n",
    "La matrice de confusion peut également être tracée à l’aide d’outils SynapseML. Cela est illustré dans l’exemple Détection des fraudes disponible [ici](https://aka.ms/samples/frauddectection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import rc, rcParams\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n",
    "    plt.yticks(tick_marks, classes, color=\"blue\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb31aa",
   "metadata": {},
   "source": [
    "Matrice de confusion pour l’outil de classification de la forêt d’arbres décisionnels avec une profondeur maximale de 4 et 4 caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_val, y_pred=ypred_rfc1_sm_v1)\n",
    "plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n",
    "                      title='Random Forest with max depth of 4')\n",
    "tn, fp, fn, tp = cfm.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c261f55",
   "metadata": {},
   "source": [
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-train-models/random-forest-4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e4937",
   "metadata": {},
   "source": [
    "Matrice de confusion pour l’outil de classification de la forêt d’arbres décisionnels avec une profondeur maximale de 8 et 6 caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_val, y_pred=ypred_rfc2_sm_v1)\n",
    "plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n",
    "                      title='Random Forest with max depth of 8')\n",
    "tn, fp, fn, tp = cfm.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cdcf6",
   "metadata": {},
   "source": [
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-train-models/random-forest-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346966a9",
   "metadata": {},
   "source": [
    "Matrice de confusion pour LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485694c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_val, y_pred=ypred_lgbm1_sm_v1)\n",
    "plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n",
    "                      title='LightGBM')\n",
    "tn, fp, fn, tp = cfm.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354ea53",
   "metadata": {},
   "source": [
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-train-models/lightgbm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2230",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5c02a",
   "metadata": {},
   "source": [
    "## Etape 13: Notations des lots et sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7ff4a",
   "metadata": {},
   "source": [
    "Chargez les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.format(\"delta\").load(\"Tables/df_test\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01787cdf",
   "metadata": {},
   "source": [
    "Pour utiliser l’API Transformer à partir de SynapseML, vous devez d’abord créer un objet MLFlowTransformer.\n",
    "\n",
    "L’objet MLFlowTransformer est un wrapper autour du modèle MLFlow que vous avez inscrit dans la partie 3. Il vous permet de générer des prédictions par lots sur un DataFrame donné. Pour instancier l’objet MLFlowTransformer, vous devez fournir les paramètres suivants :\n",
    "\n",
    "Colonnes du DataFrame de test dont vous avez besoin pour les utiliser comme entrée dans le modèle (dans ce cas, vous aurez besoin de toutes).\n",
    "Nom de la nouvelle colonne de sortie (dans ce cas, prédictions).\n",
    "Nom et version de modèle corrects pour générer les prédictions (dans ce cas, lgbm_sm et version 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70513b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.predict import MLFlowTransformer\n",
    "\n",
    "model = MLFlowTransformer(\n",
    "    inputCols=list(df_test.columns),\n",
    "    outputCol='predictions',\n",
    "    modelName='lgbm_sm',\n",
    "    modelVersion=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba925a",
   "metadata": {},
   "source": [
    "Maintenant que vous disposez de l’objet MLFlowTransformer, vous pouvez l’utiliser pour générer des prédictions par lots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "predictions = model.transform(df_test)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5ca02",
   "metadata": {},
   "source": [
    "Le code suivant appelle la fonction PREDICT avec l’API SQL Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c68d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer \n",
    "\n",
    "# Substitute \"model_name\", \"model_version\", and \"features\" below with values for your own model name, model version, and feature columns\n",
    "model_name = 'lgbm_sm'\n",
    "model_version = 1\n",
    "features = df_test.columns\n",
    "\n",
    "sqlt = SQLTransformer().setStatement( \n",
    "    f\"SELECT PREDICT('{model_name}/{model_version}', {','.join(features)}) as predictions FROM __THIS__\")\n",
    "\n",
    "# Substitute \"X_test\" below with your own test dataset\n",
    "display(sqlt.transform(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998267af",
   "metadata": {},
   "source": [
    "Le code suivant appelle la fonction PREDICT avec une fonction UDF PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8791e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, udf, lit\n",
    "\n",
    "# Substitute \"model\" and \"features\" below with values for your own model name and feature columns\n",
    "my_udf = model.to_udf()\n",
    "features = df_test.columns\n",
    "\n",
    "display(df_test.withColumn(\"predictions\", my_udf(*[col(f) for f in features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e22a3",
   "metadata": {},
   "source": [
    "Notez que vous pouvez également générer du code PREDICT à partir de la page d’élément d’un modèle. En savoir plus sur [PREDICT](https://aka.ms/fabric/predict-from-model-item).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6fffa",
   "metadata": {},
   "source": [
    "Une fois que vous avez généré des prédictions par lots, réécrivez les résultats de prédiction du modèle dans le lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to lakehouse to be used for generating a Power BI report\n",
    "table_name = \"customer_churn_test_predictions\"\n",
    "predictions.write.format('delta').mode(\"overwrite\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark DataFrame saved to delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d61f3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44629c4b",
   "metadata": {},
   "source": [
    "## Etape 14: Model ML et Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cbb64",
   "metadata": {},
   "source": [
    "Créez un nouveau modèle sémantique associé aux données de prédictions que vous avez générées dans la partie 4 :\n",
    "\n",
    "1. Sur la gauche, sélectionnez votre espace de travail.\n",
    "2. En haut à gauche, sélectionnez Lakehouse comme filtre.\n",
    "3. Sélectionnez le lakehouse que vous avez utilisé dans les parties précédentes de la série de tutoriels.\n",
    "4. Sélectionnez Nouveau modèle sémantique dans le ruban supérieur.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/new-power-bi-dataset.png)\n",
    "\n",
    "5. Nommez le modèle sémantique, par exemple « prédictions de l’attrition bancaire. » Sélectionnez ensuite le jeu de données customer_churn_test_predictions.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/select-predictions-data.png)\n",
    "\n",
    "6. Cliquez sur Confirmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db172f29",
   "metadata": {},
   "source": [
    "Ajoutez à présent quelques mesures au modèle sémantique :\n",
    "\n",
    "1. Ajoutez une nouvelle mesure pour l’attrition.\n",
    "\n",
    "  * Sélectionnez Nouvelle mesure dans le ruban supérieur. Cette action ajoute un nouvel élément nommé Mesure au jeu de données customer_churn_test_predictions et ouvre une barre de formules au-dessus du tableau.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/new-measure.png)\n",
    "\n",
    "  * Pour déterminer la prédiction de taux moyen d’attrition, remplacez Measure = dans la barre de formule par :\n",
    "  ```Churn Rate = AVERAGE(customer_churn_test_predictions[predictions])```\n",
    "  *  Pour appliquer la formule, sélectionnez la coche dans la barre de formule. La nouvelle mesure apparaît dans la table de données. L’icône de calculatrice montre qu’elle a été créée en tant que mesure.\n",
    "  *  Remplacez le format Général par Pourcentage dans le panneau Propriétés.\n",
    "  *  Faites défiler vers le bas dans le panneau Propriétés pour modifier les décimales pour 1.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/churn-rate.png)\n",
    "\n",
    "2. Ajoutez une nouvelle mesure qui compte le nombre total de clients de banque. Vous en aurez besoin pour le reste des nouvelles mesures.\n",
    "\n",
    "  * Sélectionnez Nouvelle mesure dans le ruban supérieur pour ajouter un nouvel élément nommé Mesure au jeu de données customer_churn_test_predictions. Cette action ouvre également une barre de formule au-dessus de la table.\n",
    "  *  Chaque prédiction représente un client. Pour déterminer le nombre total de clients, remplacez Measure = dans la barre de formule par : \\\n",
    "  ```Customers = COUNT(customer_churn_test_predictions[predictions])```\n",
    "  *  Pour appliquer la formule, sélectionnez la coche dans la barre de formule.\n",
    "\n",
    "3. Ajoutez l’attrition pour l’Allemagne.\n",
    "\n",
    "  * Sélectionnez Nouvelle mesure dans le ruban supérieur pour ajouter un nouvel élément nommé Mesure au jeu de données customer_churn_test_predictions. Cette action ouvre également une barre de formule au-dessus de la table.\n",
    "  * Pour déterminer l’attrition pour l’Allemagne, remplacez Measure = dans la barre de formule par :\\\n",
    "    ```Germany Churn = CALCULATE(AVERAGE(customer_churn_test_predictions[predictions]),FILTER(customer_churn_test_predictions, customer_churn_test_predictions[Geography_Germany] = TRUE()))``` \\\n",
    "    Cela filtre les lignes vers le bas jusqu’à celles avec l’Allemagne comme région géographique (Geography_Germany est égal à un).\n",
    "  * Pour appliquer la formule, sélectionnez la coche dans la barre de formule.\n",
    "\n",
    "4. Répétez l’étape ci-dessus pour ajouter l’attrition pour la France et l’Espagne.\n",
    "\n",
    "  * Attrition pour l’Espagne : \\\n",
    "  ```Spain Churn = CALCULATE(AVERAGE(customer_churn_test_predictions[predictions]),FILTER(customer_churn_test_predictions, customer_churn_test_predictions[Geography_Spain] = TRUE()))```\n",
    "  * Attrition pour la France : \\\n",
    "  ```France Churn = CALCULATE(AVERAGE(customer_churn_test_predictions[predictions]),FILTER(customer_churn_test_predictions, customer_churn_test_predictions[Geography_France] = TRUE()))```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d616f",
   "metadata": {},
   "source": [
    "Une fois que vous avez terminé avec toutes les opérations, accédez à la page de création de rapports Power BI en sélectionnant Créer un rapport dans le ruban supérieur.\n",
    "\n",
    "![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/visualize-this-data.png)\n",
    "\n",
    "Une fois la page de rapport affichée, ajoutez les objets visuels suivants :\n",
    "\n",
    "  1. Sélectionnez la zone de texte dans le ruban supérieur et entrez un titre pour le rapport, par exemple « Taux d’attrition de clients de banque ». Modifiez la taille de police et la couleur d’arrière-plan dans le volet Format. Ajustez la taille et la couleur de police en sélectionnant le texte et en utilisant la barre de mise en forme.\n",
    "\n",
    "  2. Dans le volet Visualisations, sélectionnez l’icône Carte. Dans le volet Données, sélectionnez Attrition. Modifiez la taille de police et  la couleur d’arrière-plan dans le volet Format. Faites glisser cette visualisation en haut à droite du rapport.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/card-churn.png)\n",
    "\n",
    "  3. Dans le volet Visualisations, sélectionnez l’icône Graphique en courbes et en colonnes empilées. Sélectionnez Âge pour l’axe X, Attrition pour l’axe Y de colonne et Clients pour l’axe Y de ligne.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/age.png)\n",
    "\n",
    "  4. Dans le volet Visualisations, sélectionnez l’icône Graphique en courbes et en colonnes empilées. Sélectionnez NumOfProducts pour l’axe X, Attrition pour l’axe Y de colonne et Clients pour l’axe Y de ligne.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/number-of-products.png)\n",
    "\n",
    "  5. Dans le volet Visualisations, sélectionnez l’icône Graphique en colonnes empilées. Sélectionnez NewCreditsScore pour l’axe X et Attrition pour l’axe Y.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/new-credit-score.png)\n",
    "\n",
    "     Remplacez le titre « NewCreditsScore » par « Score de crédit » dans le volet Mise en forme.\n",
    "     ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/change-title.png)\n",
    "\n",
    "  6. Dans le volet Visualisations, sélectionnez le graphique Histogramme groupé. Sélectionnez Attrition pour l’Allemagne, Attrition pour l’Espagne et Attrition pour la France, dans cet ordre, pour l’axe Y.\n",
    "\n",
    "  ![image-alt-text](https://learn.microsoft.com/fr-fr/fabric/data-science/media/tutorial-data-science-create-report/germany-spain-france.png)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a818f6",
   "metadata": {},
   "source": [
    "Conclusion du rapport Power BI :\n",
    "\n",
    "Les clients d’une banque qui utilisent plus de deux des produits bancaires ont un taux d’attrition plus élevé, \\\n",
    "bien que peu de clients aient plus de deux produits. La banque doit collecter plus de données, \\\n",
    "mais également examiner d’autres fonctionnalités mises en corrélation avec davantage de produits (consulter le tracé dans le volet inférieur gauche). \\\n",
    "Les clients d’une banque en Allemagne ont une attrition plus élevée qu’en France et en Espagne (consulter le tracé dans le panneau inférieur droit), \\\n",
    "ce qui suggère qu’une enquête sur ce qui a favorisé le départ des clients pourrait être utile. \\\n",
    "Il y a plus de clients d’âge moyen (entre 25 et 45 ans) et les clients entre 45 et 60 ans ont davantage tendance à quitter la banque. \\\n",
    "Enfin, les clients dont la cote de crédit est plus faible quitteront très probablement la banque pour d’autres institutions financières. \\\n",
    "La banque doit examiner les moyens qui encouragent les clients ayant des scores de crédit et des soldes de compte plus faibles à rester avec la banque."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "fr"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
